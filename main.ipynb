{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from importlib-metadata) (3.16.2)\n",
      "Collecting soundfileNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Using cached cffi-1.15.1-cp310-cp310-win_amd64.whl (179 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, cffi, soundfile\n",
      "Successfully installed cffi-1.15.1 pycparser-2.21 soundfile-0.12.1\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\fujitsu\\\\Desktop\\\\Georgia\\\\CS7643_DeepLearning\\\\whisper\\\\.conda\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas64__v0.3.23-gcc_10_3_0.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from librosa) (1.25.1)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Using cached scipy-1.11.1-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "Collecting scikit-learn>=0.20.0 (from librosa)\n",
      "  Using cached scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Using cached joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Using cached numba-0.57.1-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Collecting pooch<1.7,>=1.0 (from librosa)\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.3.5-cp310-cp310-win_amd64.whl (184 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from librosa) (4.7.1)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Using cached msgpack-1.0.5-cp310-cp310-win_amd64.whl (61 kB)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Using cached llvmlite-0.40.1-cp310-cp310-win_amd64.whl (27.7 MB)\n",
      "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 (from librosa)\n",
      "  Using cached numpy-1.24.4-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Collecting appdirs>=1.3.0 (from pooch<1.7,>=1.0->librosa)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (2.31.0)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->librosa)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2023.7.22)\n",
      "Installing collected packages: msgpack, appdirs, threadpoolctl, numpy, llvmlite, lazy-loader, joblib, audioread, soxr, scipy, pooch, numba, scikit-learn, librosa\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "Collecting jiwerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
      "Collecting click<9.0.0,>=8.1.3 (from jiwer)\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\n",
      "Collecting rapidfuzz==2.13.7 (from jiwer)\n",
      "  Using cached rapidfuzz-2.13.7-cp310-cp310-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Installing collected packages: rapidfuzz, click, jiwer\n",
      "Successfully installed click-8.1.6 jiwer-3.0.2 rapidfuzz-2.13.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/adapter-transformers'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\fujitsu\\desktop\\georgia\\cs7643_deeplearning\\whisper\\.conda\\lib\\site-packages (from importlib-metadata) (3.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers\n",
    "%pip install -U accelerate\n",
    "%pip install importlib-metadata\n",
    "%pip install soundfile\n",
    "%pip install librosa\n",
    "%pip install jiwer\n",
    "%pip install evaluate\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.utils.versions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m WhisperConfig\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madapters\u001b[39;00m \u001b[39mimport\u001b[39;00m WhisperModelWithHeads\n\u001b[0;32m      4\u001b[0m \u001b[39m#Defining the configuration for the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fujitsu\\Desktop\\Georgia\\CS7643_DeepLearning\\adapter-transformers\\src\\transformers\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[39m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     33\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fujitsu\\Desktop\\Georgia\\CS7643_DeepLearning\\adapter-transformers\\src\\transformers\\dependency_versions_check.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdependency_versions_table\u001b[39;00m \u001b[39mimport\u001b[39;00m deps\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversions\u001b[39;00m \u001b[39mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     20\u001b[0m \u001b[39m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# order specific notes:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[1;32mc:\\Users\\fujitsu\\Desktop\\Georgia\\CS7643_DeepLearning\\adapter-transformers\\src\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdoc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     can_return_loss,\n\u001b[0;32m     42\u001b[0m     expand_dims,\n\u001b[0;32m     43\u001b[0m     find_labels,\n\u001b[0;32m     44\u001b[0m     flatten_dict,\n\u001b[0;32m     45\u001b[0m     is_jax_tensor,\n\u001b[0;32m     46\u001b[0m     is_numpy_array,\n\u001b[0;32m     47\u001b[0m     is_tensor,\n\u001b[0;32m     48\u001b[0m     is_tf_tensor,\n\u001b[0;32m     49\u001b[0m     is_torch_device,\n\u001b[0;32m     50\u001b[0m     is_torch_dtype,\n\u001b[0;32m     51\u001b[0m     is_torch_tensor,\n\u001b[0;32m     52\u001b[0m     reshape,\n\u001b[0;32m     53\u001b[0m     squeeze,\n\u001b[0;32m     54\u001b[0m     tensor_size,\n\u001b[0;32m     55\u001b[0m     to_numpy,\n\u001b[0;32m     56\u001b[0m     to_py_obj,\n\u001b[0;32m     57\u001b[0m     transpose,\n\u001b[0;32m     58\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     61\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     62\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m     send_example_telemetry,\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     91\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     92\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m     torch_version,\n\u001b[0;32m    170\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fujitsu\\Desktop\\Georgia\\CS7643_DeepLearning\\adapter-transformers\\src\\transformers\\utils\\generic.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m is_tf_available():\n\u001b[0;32m     33\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fujitsu\\Desktop\\Georgia\\CS7643_DeepLearning\\adapter-transformers\\src\\transformers\\utils\\import_utils.py:32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m \u001b[39mimport\u001b[39;00m version\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversions\u001b[39;00m \u001b[39mimport\u001b[39;00m importlib_metadata\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m     37\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mget_logger(\u001b[39m__name__\u001b[39m)  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.utils.versions'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import accelerate\n",
    "from transformers import WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor\n",
    "from src.transformers.models.whisper.modeling_whisper import WhisperForConditionalGeneration\n",
    "from src.transformers.adapters import AdapterConfig\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\")\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\")\n",
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print('###############Loading Dataset Is Complete################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract, Tokenize and Process\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"Hindi\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")\n",
    "\n",
    "print('###############Extract, Tokenize and Process Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "print('###############Data Preparation Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "print('###############Data Collator Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print('###############Compute metrics Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the adapter layers\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "adapter_config = AdapterConfig.load(\"pfeiffer\")\n",
    "model.add_adapter(\"transcribe\")\n",
    "\n",
    "print('###############Adapter Inclusion Is Complete################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your training arguments and data\n",
    "training_args = TrainingArguments(\n",
    "    #max_steps=80000,\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False\n",
    "    #save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create a Trainer and train the model on your task-specific dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print('###############Training Arguments placing Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "print('###############Training Is Complete################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python adapter.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
